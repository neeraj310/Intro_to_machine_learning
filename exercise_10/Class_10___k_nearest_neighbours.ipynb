{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 10 - The k-nearest neighbour algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $k$-nearest neighbour algorithm is probably one of the most simple classification algorithm existing. The general idea is to predict the label of a new datapoint according to the label(s) of the closest training point(s) to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "x, y = make_moons(50, noise=0.2, random_state=5)#random_state=5\n",
    "\n",
    "plt.scatter(x[:,0], x[:,1], c=y)\n",
    "plt.scatter([1.3], [0.6],c=\"g\")\n",
    "plt.plot((1.0141,1.3),(0.6409,0.6),\"g\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, it is a good idea to simply decide the label of a newly observed datapoint according to the label of the closest given point in the training set. Unfortunately, this leads to the following two problems:\n",
    "- The algorithm is not generalizing well as points at the boundary get a huge say whereas points in the bulk only get a small voice.\n",
    "- Outliers contribute to the prediction as much as the \"well-located\" points. This also prevents the algorithm from generalizing well.\n",
    "\n",
    "To solve these issues one usually considers the $k$-nearest neighbours of a new point instead of only the closest given one. These points then are used to perform a majority vote with the hope that outliers do not contribute too much anymore.\n",
    "\n",
    "To implement the $k$-nearest neighbour algorithm, we simply need to store all training data in memory and while predicting search for the closest $k$ points within this set.\n",
    "\n",
    "The $k$-nearest neighbour algorithm works non-parametric which means that there is no parametrized underlying hypothesis class from which we are trying to learn. Therefore it has a huge capability of capturing non-smooth, complex decision boundaries. This is for example useful in recognizing hand-written digits sucessfully.\n",
    "\n",
    "Advantages:\n",
    "- Applies easily to multiclass classification\n",
    "- Building the model is very fast\n",
    "\n",
    "Disadvantages:\n",
    "- Prediction is very slow especially for large number of features.\n",
    "- Need to preprocess/standardize data\n",
    "- Bad in treating sparse data\n",
    "- Suffers from curse of dimensionality, i.e. needed sample size grows exponentially in number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The role of $k$\n",
    "\n",
    "More neighbours means smoother boundary and smoother decision boundary corresponds to simpler model. On the other hand, the boundary for small values of $k$ gets very rough. Hence, the decision rule gets more complicated. Put differently, low k corresponds to high model complexity and high k corresponds to low model complexity.\n",
    "\n",
    "In practice, one often starts to train the algorithm with $k=3$ or $k=5$ and adapts accordingly afterwards.\n",
    "\n",
    "In some problems it might also be helpful to let the value of $k$ to depend on the averaged local density of points in the training data. One approach to obtain this is to apply a radius-based approach (cf. RadiusNeighborsClassifyer in sklearn). The idea for this is to consider all training points within a fixed radius $r$ around the new datapoint instead of only the $k$ closest ones. The advantage compared to a classical $k$-nearest neighbour algorithm is that the influence of a given point in the training set is limited to a fixed region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to compute nearest neighbours?\n",
    "\n",
    "Finding the nearest neighbour in a fast way is usually a quite challenging problem. On the other hand, it can also be done quite fast depending on the problem - for example Google uses a nearest neighbour search in its algorithm and manages to search through 30 trillion web pages.\n",
    "\n",
    "Developing fast and storage-economical nearest neighbour search algorithms is still an active part of research. Usually, such algorithms have to perform a delicate tradeoff between the low time for query and the space complexity needed. In general, the better the data are sorted and stored, the faster one can search them but storing these complex data structures demands a lot of storage space. One option to avoid this problem (at least slightly) is by considering so-called $r$-approximate searchs only. Some of the most important nearest neighbour algorithms are:\n",
    "\n",
    "*Brute force*: Compute all distances and search for the smallest among all of them. The time for a query scales like $O(dn^2)$ while the needed storage scales like $O(dn)$. Make sure that calculation of your distance can be done fast.\n",
    "\n",
    "*k-d-tree*: Apply the principle “If A distant to B and C close to B than C distant from A without computing it”. Computational time scales like $O(dnlogn)$. The idea to build such a tree is to partition the data set into close and distant points. Efficent until roughly d=20.\n",
    "\n",
    "*ball-tree*: Construct tree structure not along cartesian axis but use proper balls together with the triangle inequality to get estimate for distrances. This algorithm provides much faster prediction rates but needs very long to build the data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "x, y = make_moons(300, noise=0.4, random_state=5)\n",
    "\n",
    "plt.scatter(x[:,0], x[:,1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prediction we decided to implement the brute force method. This method works best for a small number of samples and low feature dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eucl_dist(x, y, axis=None):\n",
    "    return np.linalg.norm(x-y, axis=axis)\n",
    "\n",
    "def kNN_train(x, y, nr_neighbours=1, dist=eucl_dist):\n",
    "    # Standardize data and add column of ones\n",
    "    x_mean = np.mean(x, axis=0)\n",
    "    x_std = np.std(x, axis=0)\n",
    "    return [(x-x_mean)/x_std, x_mean, x_std, y, nr_neighbours, dist]\n",
    "\n",
    "def max_vote(y): # Y of the shape: nr_samples × nr_neighbours\n",
    "    u, indices = np.unique(y, return_inverse=True)\n",
    "    return u[np.argmax(np.apply_along_axis(np.bincount, 1, indices.reshape(y.shape), None, np.max(indices) + 1), axis=1)]\n",
    "\n",
    "def kNN_predict(x_pred, kNN):\n",
    "    x, x_mean, x_std, y, nr_neighbours, dist = kNN\n",
    "    \n",
    "    # Standardize data accordingly\n",
    "    x_pred = (x_pred-x_mean)/x_std\n",
    "    \n",
    "    # Instead of looping through all elements in x_pred we perform all calculations in parallel using numpys broadcasting\n",
    "    # Shape: nr_samples_to_predict × nr_samples × nr_features\n",
    "    x=x.reshape(1,*x.shape)\n",
    "    x_pred = x_pred.reshape(x_pred.shape[0],1,x_pred.shape[1])\n",
    "    \n",
    "    # Average over features\n",
    "    dist_x_xpred = dist(x, x_pred, axis=2)\n",
    "    \n",
    "    # Find the smallest nr_neighboughs one\n",
    "    k_nearest_position = np.argpartition(dist_x_xpred, nr_neighbours-1)\n",
    "    \n",
    "    # Select k nearest outputs for any sample_to_predict\n",
    "    # new shape: nr_samples_to_predict × nr_neighbours\n",
    "    k_nearest_outputs = np.take_along_axis(y.reshape(1,-1), k_nearest_position, axis=1)[:,:nr_neighbours]\n",
    "    \n",
    "    # Perform prediction by maximum vote.\n",
    "    return max_vote(k_nearest_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need our standard preprocessing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(x, y):\n",
    "    y = y.reshape(-1,1)\n",
    "    z = np.hstack((x, y))\n",
    "    np.random.shuffle(z)\n",
    "    x_new, y_new = np.hsplit(z, [x.shape[1]])\n",
    "    return x_new, y_new.reshape(-1)\n",
    "\n",
    "def splitting(x, y, test_size=0.2):\n",
    "    n = x.shape[0]\n",
    "    train_size = int(n * (1 - test_size))\n",
    "    return x[:train_size, ], x[train_size:, ], y[:train_size, ], y[train_size:, ]\n",
    "\n",
    "def validation_split(x, y, validation_size=0.15, test_size=0.15):\n",
    "    n = x.shape[0]\n",
    "    train_cut = int(np.floor(n * (1 - validation_size - test_size)))\n",
    "    val_cut = train_cut + int(np.floor(n * validation_size))\n",
    "    return x[:train_cut, ], x[train_cut:val_cut, ], x[val_cut:, ], y[:train_cut, ], y[train_cut:val_cut, ], y[val_cut:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_one_loss(y_true, y_pred):\n",
    "    n = y_true.shape[0]\n",
    "    return (1/n) * np.sum(y_true != y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are able to apply our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = shuffle(x, y)\n",
    "x_train, x_test, y_train, y_test = splitting(x, y)\n",
    "\n",
    "\n",
    "kNN = kNN_train(x_train, y_train, nr_neighbours=1)\n",
    "\n",
    "train_predicted = kNN_predict(x_train, kNN)\n",
    "test_predicted = kNN_predict(x_test, kNN)\n",
    "\n",
    "print('Train 0-1 loss:', zero_one_loss(y_train, train_predicted))\n",
    "print('Test 0-1 loss:', zero_one_loss(y_test, test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already seen in the non-linear SVM algorithm, it is not possible to determine the prediction rule in a simple way. Thus, we draw the decision boundary to get an understanding of how our algorithm predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draws the decision boundary for a classifyer with two features\n",
    "def decision_boundary(clf, plt, x1_range, x2_range):\n",
    "    # Create grid\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_range[0], x1_range[1], 0.1),\n",
    "                     np.arange(x2_range[0], x2_range[1], 0.1))\n",
    "    \n",
    "    # Predict on all values\n",
    "    y = kNN_predict(np.c_[xx1.reshape(-1), xx2.reshape(-1)], kNN)\n",
    "    y = y.reshape(xx1.shape)\n",
    "    \n",
    "    # Plot contour\n",
    "    plt.contourf(xx1, xx2, y, alpha=0.4)\n",
    "\n",
    "# Plot both training and testing\n",
    "f, subplt = plt.subplots(1, 2, figsize=(10, 5))\n",
    "x1_range = [x[:, 0].min() - 1/2, x[:, 0].max() + 1/2]\n",
    "x2_range = [x[:, 1].min() - 1/2, x[:, 1].max() + 1/2]\n",
    "\n",
    "subplt[0].set_title(\"KNN training set\")\n",
    "decision_boundary(kNN, subplt[0], x1_range, x2_range)\n",
    "subplt[0].scatter(x_train[:, 0], x_train[:, 1], c=y_train, edgecolor='k')\n",
    "\n",
    "subplt[1].set_title(\"KNN test set\")\n",
    "decision_boundary(kNN, subplt[1], x1_range, x2_range)\n",
    "subplt[1].scatter(x_test[:, 0], x_test[:, 1], c=y_test, edgecolor='k')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the decision boundary is very rough so that it probably generalizes poorly. Let's compare the boundary for different levels of $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing different levels of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k_list=[1,2,3,4,5,6,7,8,9,10,20,30,40,50]\n",
    "\n",
    "# Plot both training and testing\n",
    "f, subplt = plt.subplots(len(k_list), 2, figsize=(10, 5*len(k_list)))\n",
    "x1_range = [x[:, 0].min() - 0.5, x[:, 0].max() + 0.5]\n",
    "x2_range = [x[:, 1].min() - 0.5, x[:, 1].max() + 0.5]\n",
    "\n",
    "for i in range(len(k_list)):\n",
    "    kNN = kNN_train(x_train, y_train, nr_neighbours=k_list[i])\n",
    "\n",
    "    train_predicted = kNN_predict(x_train, kNN)\n",
    "    test_predicted = kNN_predict(x_test, kNN)\n",
    "\n",
    "    subplt[i,0].set_title(\"kNN training set k=\"+str(k_list[i])\n",
    "                          +\"\\n train error =\"+str(zero_one_loss(y_train, train_predicted))[:5])\n",
    "    decision_boundary(kNN, subplt[i,0], x1_range, x2_range)\n",
    "    subplt[i,0].scatter(x_train[:, 0], x_train[:, 1], c=y_train, edgecolor='k')\n",
    "    \n",
    "\n",
    "    subplt[i,1].set_title(\"kNN test set k=\"+str(k_list[i])\n",
    "                          +\"\\n test error =\"+str(zero_one_loss(y_test, test_predicted))[:5])\n",
    "    decision_boundary(kNN, subplt[i,1], x1_range, x2_range)\n",
    "    subplt[i,1].scatter(x_test[:, 0], x_test[:, 1], c=y_test, edgecolor='k')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to a “real” problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us investigate how the $k$-nearest neighbour algorithm performs on a real dataset. We consider here the [wine dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html) from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = wine.data\n",
    "y = wine.target\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the dataset is quite small we decided to perform a standard validation and compare it to the leave one out cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "#np.random.seed(5)\n",
    "x, y = shuffle(x, y)\n",
    "\n",
    "x_train, x_validation, x_test, y_train, y_validation, y_test = validation_split(x, y)\n",
    "# Doing a normal validation might be not so efficient as the dataset is quite small.\n",
    "# So we also perform cross-validation (leave one out).\n",
    "x_train_cv = np.vstack((x_train,x_validation))\n",
    "y_train_cv = np.hstack((y_train,y_validation))\n",
    "\n",
    "train_errors = []\n",
    "validation_errors = []\n",
    "train_errors_cv = []\n",
    "validation_errors_cv = []\n",
    "\n",
    "for i in range(1,50):\n",
    "    # Standard validation\n",
    "    kNN = kNN_train(x_train, y_train, nr_neighbours=i)\n",
    "    train_predicted = kNN_predict(x_train, kNN)\n",
    "    validation_predicted = kNN_predict(x_validation, kNN)\n",
    "    train_errors.append(zero_one_loss(y_train, train_predicted))\n",
    "    validation_errors.append(zero_one_loss(y_validation, validation_predicted))\n",
    "    \n",
    "    #Leave one out cross validation\n",
    "    loo = LeaveOneOut()\n",
    "    train_errors_loo = []\n",
    "    validation_errors_loo = []\n",
    "    for train_index, test_index in loo.split(x_train_cv):\n",
    "        x_train_loo = x_train_cv[train_index,:]\n",
    "        y_train_loo = y_train_cv[train_index]\n",
    "        x_validation_loo = x_train_cv[test_index,:]\n",
    "        y_validation_loo = y_train_cv[test_index]\n",
    "        \n",
    "        kNN_loo = kNN_train(x_train_loo, y_train_loo, nr_neighbours=i)\n",
    "        train_predicted_loo = kNN_predict(x_train_loo, kNN_loo)\n",
    "        validation_predicted_loo = kNN_predict(x_validation_loo, kNN_loo)\n",
    "        train_errors_loo.append(zero_one_loss(y_train_loo, train_predicted_loo))\n",
    "        validation_errors_loo.append(zero_one_loss(y_validation_loo, validation_predicted_loo))\n",
    "    train_errors_cv.append(sum(train_errors_loo)/len(train_errors_loo))\n",
    "    validation_errors_cv.append(sum(validation_errors_loo)/len(validation_errors_loo))\n",
    "\n",
    "plt.title(\"Training (b) and validation error (r) using standard validation\")\n",
    "plt.plot(train_errors, \"b\")\n",
    "plt.plot(validation_errors, \"r\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Training (b) and validation error (r) using leave one out\")\n",
    "plt.plot(train_errors_cv, \"b\")\n",
    "plt.plot(validation_errors_cv, \"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us see how we can use the $k$-nearest neighbour algorithm as implented in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=i).fit(x_train, y_train)\n",
    "score = clf.score(x_validation, y_validation)\n",
    "    \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Think about how to draw decision boundaries for the wine dataset. Can you see something when looking at the dataset?\n",
    "2. Compare the performance of the $k$-nearest neighbour algorithm with the other classifiers you have seen, e.g. on the titanic problem.\n",
    "3. Adapt the idea of the $k$-nearest neighbour algorithm to implement a $k$-nearest neighbour regressor. Apply it to a toy problem as well as to \"real\" problem like the Boston house price dataset to investigate its performance.\n",
    "4. Implement weights in our $k$-nearest neighbour algorithm so that closer points contribute more in the prediction than points further away. Compare this new algorithm to the standard $k$-nearest neighbour algorithm without weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
