{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 7 - Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized loss minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Regularized loss minimization (RLM)* is a learning rule in which we minimize the empirical risk together with a regularization function $\\rho : \\mathbb{R}^d \\to \\mathbb{R}$, which often leads to slightly higher bias but significantly reduced variance in the bias-variance tradeoff.\n",
    "\n",
    "If the hypothesis class is parametrized by $ \\mathbf{w} \\in \\mathbb{R}^d$, then the regularized loss minimization rule outputs a hypothesis in \n",
    "\n",
    "$$\\arg \\min_{\\mathbf{w} \\in \\mathbb{R}^d} \\big(\\hat{R}(\\mathbf{w}) + \\rho(\\mathbf{w})\\big).$$\n",
    "\n",
    "The regularization function $\\rho$ is typically chosen to penalize how complex the hypothesis is, in order to prevent overfitting. We will consider the most widely used types, the L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking $\\rho(\\mathbf{w}) = \\lambda \\| \\mathbf{w} \\|_2^2$, where $\\lambda > 0$ is a scalar and the norm is $ \\| \\mathbf{w} \\|_2 = \\big(\\sum_{j=1}^{d} (w^{(j)})^2\\big)^{1/2}$, we have the learning rule \n",
    "\n",
    "$$\\arg \\min_{\\mathbf{w} \\in \\mathbb{R}^d} \\big(\\hat{R}(\\mathbf{w}) + \\lambda \\| \\mathbf{w} \\|_2^2 \\big),$$\n",
    "\n",
    "called *L2* (or *Tikhonov*) *regularization*. An equivalent way to write this problem is \n",
    "\n",
    "$$\\arg \\min_{\\mathbf{w} \\in \\mathbb{R}^d} \\hat{R}(\\mathbf{w}) \\quad \\text{subject to} \\ \\sum_{j=1}^{d} \\big(w^{(j)}\\big)^2 \\leq C,$$\n",
    "\n",
    "which makes explicit the size constraint on the parameters.\n",
    "\n",
    "### Ridge regression\n",
    "\n",
    "Applying the RLM with L2 regularization to linear regression with the squared loss, called *ridge regression*, we obtain the learning rule\n",
    "\n",
    "$$ \\arg \\min_{β \\in \\mathbb{R}^d} \\bigg( \\lambda \\| β \\|_2^2  + \\frac{1}{n} \\sum_{i=1}^{n} \\big(y_i - \\langle β , x_i \\rangle \\big)^2 \\bigg).$$ \n",
    "\n",
    "To solve the minimization problem, we set the gradient equal to zero and obtain \n",
    "\n",
    "$$ (\\lambda n I + X^T X) β = X^T y.$$\n",
    "\n",
    "The matrix $\\lambda n I + X^T X$ is invertible since it is positive definite for $\\lambda > 0$, and we have the solution\n",
    "\n",
    "$$ β =(\\lambda n I + X^T X)^{-1}X^T y.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, model_selection, linear_model, metrics \n",
    "\n",
    "def L2_regularized_risk(X, y, l, beta):\n",
    "    return l * (np.linalg.norm(beta)**2) + (1 / X.shape[0]) * (np.linalg.norm((X.dot(beta) - y) ** 2))\n",
    "\n",
    "def ridge_regression_inv(X, y, l=1.0):\n",
    "    n, d = X.shape\n",
    "    a = (l * n) * np.identity(d) + np.transpose(X).dot(X)\n",
    "    b = np.transpose(X).dot(y)\n",
    "    beta = np.linalg.inv(a).dot(b)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the L2 norm is differentiable, learning problems using L2 regularization can also be solved by using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_grad(X, y, l=1, alpha=0.1):\n",
    "    n, d = X.shape\n",
    "    result = np.zeros((d, 1))\n",
    "    cost_new = L2_regularized_risk(X, y, l, result)\n",
    "    cost_old = 0\n",
    "    i = 0\n",
    "    while (cost_new - cost_old) ** 2 > 10 ** (-10):\n",
    "        gradient = (np.transpose(X)).dot(X.dot(result) - y) + (l * n) * np.identity(d).dot(result) ## Gradient updated for the regularization.\n",
    "        alpha = (np.transpose(gradient).dot(gradient)) / (\n",
    "            (np.transpose(gradient).dot(np.transpose(X))).dot(X.dot(gradient)))\n",
    "        result -= (2 * alpha / n) * gradient\n",
    "        cost_new, cost_old = L2_regularized_risk(X, y, l, result), cost_new\n",
    "        i += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ridge solutions are not equivariant under scaling of the inputs, so one standardizes the inputs before solving the minimization problem. This means that we take\n",
    "\n",
    "\\begin{align}\n",
    "x_{new} = \\frac{x_{old}−x_{mean}}{x_{sd}}, \\quad y_{new} = \\frac{y_{old}−y_{mean}}{y_{sd}},\n",
    "\\end{align}\n",
    "\n",
    "and arrange our learning algorithm accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def ridge_regression_inv_std(X, y, l=1.0):\n",
    "    ## Standardize data\n",
    "    def standardize(X, mean_dataset, std_dataset):\n",
    "        return (X-mean_dataset)/std_dataset\n",
    "    \n",
    "    x_mean = np.mean(X, axis=0)\n",
    "    x_std = np.std(X, axis=0)\n",
    "    y_mean = np.mean(y, axis=0)\n",
    "    y_std = np.std(y, axis=0)\n",
    "    \n",
    "    X_new = standardize(X, x_mean, x_std)\n",
    "    y_new = standardize(y, y_mean, y_std)\n",
    "    \n",
    "    ## The algorithm without standardization\n",
    "    n, d = X_new.shape\n",
    "    a = (l * n) * np.identity(d) + np.transpose(X_new).dot(X_new)\n",
    "    b = np.transpose(X_new).dot(y_new)\n",
    "    \n",
    "    ## Renormalize beta\n",
    "    beta_j = y_std * (np.linalg.inv(a).dot(b) / x_std.reshape(-1,1))\n",
    "    beta_zero = y_mean - np.sum(beta_j*x_mean.reshape(-1,1))\n",
    "    \n",
    "    return np.vstack((beta_zero, beta_j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Scikit-learn, the **sklearn.linear_model** module has a built-in class for ridge regression with different computational solver possibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "clf = Ridge(alpha=1.0, fit_intercept=True, normalize=True, solver='auto') \n",
    "            ## alpha corresponds to the regularization strength."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization hyperparameter\n",
    "\n",
    "Note that the regularization coefficient $\\lambda$ is a hyperparameter for the ridge regression. If $\\lambda = 0$, we have the standard non-regularized linear regression model. On the other hand, setting $\\lambda$ to a high value, the L2 norm of $\\beta$ will be heavily constrained, leading to a simpler model that may underfit.\n",
    "\n",
    "To see the effect of putting a penalty on the L2 norm, let us try to fit a high degree polynomial given a simple dataset. Note that the oscillations in the predictors are getting smaller and the models are becoming simpler for higher regularization coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#np.random.seed(seed=781)\n",
    "float_formatter = lambda x: \"%.8f\" % x\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "\n",
    "xp = np.linspace(-2, 2, 20)\n",
    "yp = 10* xp ** 3 - 40 * xp + np.random.normal(0, 4, 20)\n",
    "xp, yp = xp.reshape(-1,1), yp.reshape(-1,1)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "def extend_sci(x, p): \n",
    "    poly = PolynomialFeatures(degree = p)\n",
    "    return poly.fit_transform(x)\n",
    "\n",
    "p=20 ## Degree of the polynomial.\n",
    "a = np.linspace(min(xp),max(xp),num=1000).reshape(-1,1)\n",
    "log_coef = np.linspace(-11, 1, 13) ## You may change the interval for zooming in!\n",
    "coef = 10.0**log_coef \n",
    "for l in coef:\n",
    "    plt.plot(xp, yp, 'ob')\n",
    "    betap = ridge_regression_inv_std(extend_sci(xp, p)[:,1:], yp, l)\n",
    "    b = extend_sci(a, p).dot(betap)\n",
    "    plt.title('lambda = {}'.format(l))\n",
    "    plt.ylim(-50,50)\n",
    "    plt.plot(a, b, 'r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example, let us consider the Boston house-prices dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()\n",
    "x_raw, y_raw = boston.data, boston.target\n",
    "y_raw = y_raw.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wil try to fit a third degree polynomial to the normalized dataset by using ridge regression. Note that the number of features in the extended array becomes greater than the number of available samples, which may lead to overfitting in the standard linear regression. We also need to remove the first column (containing 1s) after transforming our matrix since we need to exclude the intercept in our ridge algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_poly = extend_sci(x_raw, 3) \n",
    "x_poly = np.delete(x_poly, 0, 1) ## Removes the first column consisting of 1s.\n",
    "print(x_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us prepare our data by shuffling and splitting into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(x, y):\n",
    "    z = np.hstack((x, y))\n",
    "    np.random.shuffle(z)\n",
    "    return np.hsplit(z, [x.shape[1]])\n",
    "\n",
    "x_poly, y = shuffle(x_poly, y_raw)\n",
    "\n",
    "def validation_split(x, y, validation_size=0.15, test_size=0.15):\n",
    "    n = x.shape[0]\n",
    "    train_cut = int(np.floor(n * (1 - validation_size - test_size)))\n",
    "    val_cut = train_cut + int(np.floor(n * validation_size))\n",
    "    return x[:train_cut, ], x[train_cut:val_cut, ], x[val_cut:, ], y[:train_cut, ], y[train_cut:val_cut, ], y[val_cut:,]\n",
    "\n",
    "x_train, x_validation, x_test, y_train, y_validation, y_test = validation_split(x_poly, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may now use the validation set to compare the outcome for different values of $\\lambda$ for hyperparameter selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Regularization coefficients in log-scale.\n",
    "log_coef = np.linspace(-10, 5, 100) ## You may change the interval for zooming in!\n",
    "coef = 10.0**log_coef \n",
    "\n",
    "reg_risk = []\n",
    "reg_val_error = []\n",
    "reg_R2 = []\n",
    "\n",
    "def addones(x): ## Function for adding back the 1s we removed.\n",
    "    x_add = np.hstack((np.ones((x.shape[0],1)), x))\n",
    "    return x_add\n",
    "\n",
    "for l in coef:\n",
    "    beta = ridge_regression_inv_std(x_train, y_train, l) ## Parameters of our ridge regression algorithm.\n",
    "        \n",
    "    train_error = metrics.mean_squared_error(y_train, addones(x_train).dot(beta))\n",
    "    reg_risk.append(train_error)\n",
    "    \n",
    "    val_error  = metrics.mean_squared_error(y_validation, addones(x_validation).dot(beta))\n",
    "    reg_val_error.append(val_error)\n",
    "    \n",
    "    R2 = 1 - val_error / np.var(y_validation)\n",
    "    reg_R2.append(R2)\n",
    "\n",
    "    \n",
    "\n",
    "plt.title('Training error')\n",
    "plt.ylim(0,100)\n",
    "plt.plot(log_coef, reg_risk)\n",
    "plt.show()\n",
    "\n",
    "plt.title('Least squares error on validation set')\n",
    "plt.ylim(0,100)\n",
    "plt.plot(log_coef, reg_val_error, 'r')\n",
    "plt.show()\n",
    "\n",
    "plt.title('R2 score on validation set')\n",
    "plt.ylim(-1,1)\n",
    "plt.plot(log_coef, reg_R2, color='darkred')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the hyperparameter that minimizes the least squares error on the validation set, we can test how our model performs. Note that the regularized version performs much better compared to the standard linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_hyp = 10**(-1.5) ## Chosen from validation.\n",
    "\n",
    "## Standard linear regression for comparison.\n",
    "lin_reg = linear_model.LinearRegression(fit_intercept=True, normalize=False).fit(x_train, y_train)\n",
    "beta_lin_reg = np.vstack((lin_reg.intercept_,lin_reg.coef_.reshape(-1,1)))\n",
    "test_error_lin_reg = metrics.mean_squared_error(y_test, lin_reg.predict(x_test))\n",
    "\n",
    "## Ridge regression with the chosen hyperparameter.\n",
    "beta_ridge = ridge_regression_inv_std(x_train, y_train, l_hyp)\n",
    "test_error_ridge = metrics.mean_squared_error(y_test, addones(x_test).dot(beta_ridge))\n",
    "\n",
    "print(\"Test error without regularization:\", test_error_lin_reg)\n",
    "print(\"Test error with regularization:\", test_error_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also have a look at the parameters which were chosen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference of chosen parameters:\", beta_lin_reg - beta_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Parameter chosen by ridge:\", beta_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard way of hyperparameter selection in this setting also is to use cross-validation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 regularization - Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding L1 regularization to a linear regression problem with the squared loss yields the *lasso* algorithm, defined as\n",
    "\n",
    "$$ \\arg \\min_{β \\in \\mathbb{R}^d} \\bigg( \\lambda \\| β \\|_1  + \\frac{1}{n} \\sum_{i=1}^{n} \\big(y_i - \\langle β , x_i \\rangle \\big)^2 \\bigg),$$\n",
    "\n",
    "where we have the L1 norm $\\| β \\|_1 =\\sum_{j=1}^{d} \\vert β^{(j)} \\vert$.\n",
    "\n",
    "In practice, L1 regularization produces a sparse model that has most of its parameters $β^{(j)}$ equal to zero, provided the hyperparameter $\\lambda$ is large enough. This leads to a feature selection by deciding which features are essential for prediction and which are not. \n",
    "\n",
    "Since the L1 penalty is not differentiable, different techniques from convex analysis and optimization, such as coordinate descent or subgradient methods, are required for obtaining solutions.\n",
    "\n",
    "The lasso class in scikit-learn uses coordinate descent to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = linear_model.Lasso(alpha=0.0001, fit_intercept = True, normalize = True, max_iter=1000) \n",
    "                                                        ## More iterations may be needed for convergence\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "beta_lasso = clf.coef_.reshape(-1,1)\n",
    "print(beta_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of the parameters are set to zero in this case. Choosing a larger hyperparameter leads to fewer essential features and a simpler model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.Lasso(alpha=0.01, fit_intercept = True, normalize=True)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "beta_lasso = clf.coef_.reshape(-1,1)\n",
    "print(beta_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try to use k-fold (or leave-one-out) cross-validation for selecting the hyperparameter $\\lambda$ in ridge regression for the Boston dataset.\n",
    "2. Apply and compare the ridge regression and lasso on the diabetes dataset from scikit-learn. Try to observe the essential features from the lasso algorithm.\n",
    "3. Try to implement the code for the coordinate descent in the lasso.\n",
    "4. How would you apply L1 and L2 regularization together to linear regression with squared loss? What would be the advantages in this case? (Hint: search *elastic net regularization*.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
