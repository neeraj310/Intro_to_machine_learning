{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 5 - Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression: the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From regression to classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous two classes we have seen how to solve regression problems, in which we had to predict a continuous output, i.e. our setting was $\\mathcal{X} = \\mathbb{R}^d$ and $\\mathcal{Y} = \\mathbb{R}$. Today we will see how to solve problems of binary **classification**, in which we are interested in correctly predicting if a given input in $\\mathcal{X} = \\mathbb{R}^d$ belongs to one of two classes (i.e. $\\mathcal{Y} = \\{0,1\\}$). \n",
    "\n",
    "Today we will present **logistic regression**, which is one of the possible models that can be used to solve a binary classification problem and is conceptually close to linear regression. Of course we cannot just use linear regression for binary classification, because a linear regression model outputs response values in $\\mathbb{R}$, while we want our response to be in $\\{0,1\\}$.\n",
    "\n",
    "First of all, let's pass from affine models to linear models by adding one more feature (constant one) to our dataset ($x \\mapsto (1, x)$). From now on $d$ will denote the dimension of our extended feature space.\n",
    "\n",
    "A possible solution to our range problem is to compose the output of a linear regression with a function with a binary range, that is we could use the following hypothesis class:\n",
    "\n",
    "$$\\{x\\mapsto \\text{sign}\\left(\\langle w,x\\rangle \\right) \\;\\vert\\; w\\in\\mathbb{R}^d\\}$$\n",
    "\n",
    "and claim that if the sign of our linear regression prediction is positive, then we will label the point $1$, while if the sign is negative, we will label it $0$. \n",
    "\n",
    "This hypothesis class is well known and particularly useful for so called linearly-separable problems (we will see more on this in a following class).\n",
    "\n",
    "Logistic regression, instead, is based on the following choice of hypothesis class:\n",
    "\n",
    "$$\\{x\\mapsto f_{w}(x) = \\sigma\\left(\\langle w,x\\rangle \\right) \\;\\vert\\; w \\in \\mathbb{R}^d\\},$$\n",
    "\n",
    "where $S$ is the **sigmoid** function, defined as\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-6,6,1000)\n",
    "\n",
    "plt.plot(x, 1/(1+np.exp(-x)), 'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The maximum likelihood method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of our model will therefore not be binary, but it will lie in $[0,1]$ and we can then interpret it as the conditional probability that the correct labeling is $Y=1$, given that we observe $X=x$:\n",
    "\n",
    "$$y = f_{w}(x) = \\sigma\\left(\\langle w,x\\rangle \\right) = \\mathbb{P}(Y = 1 | X = x).$$\n",
    "\n",
    "Now, we want to choose our parameters ($w \\in \\mathbb{R}^{d}$), in such a way that this conditional probability is as close as possible to the real one, by using the information provided by our training dataset.\n",
    "\n",
    "In the case of linear regression, we decided to find the parameters by minimizing a given loss function (squared error or absolute value loss) on our training dataset. This method worked very well analytically and/or computationally because that loss function was convex in the parameters, but in the case of logistic regression the non-linearity of the sigmoid function, $\\sigma$, forces us to find another way.\n",
    "\n",
    "We can fix our parameters by imposing that **the training dataset that we observe must have the maximum probability of occurring**, in other words we want to maximize the following function:\n",
    "\n",
    "\n",
    "\n",
    "$$ \\prod_{i=1}^n \\mathbb{P}(Y=y_i, X=x_i) = \\prod_{i=1}^n \\mathbb{P}(Y=y_i | X=x_i) \\mathbb{P}(X = x_i) \\propto \\prod_{i=1}^n \\left(f_{w}(x_i)\\right)^{y_i} \\left(1 - f_{w}(x_i)\\right)^{1 - y_i} =: L(w) $$\n",
    "\n",
    "where $L(w)$ is known as the likelihood function and is proportional to the probability of observing our training dataset, under our working assumption that $f_{w}(x_i)$ must be the conditional probability.\n",
    "\n",
    "Of course maximizing $L(w)$ is equivalent to maximizing its logarithm, so that our parameters, $w$, can be found by solving:\n",
    "\n",
    "$$ \\hat{w} = \\underset{w \\in \\mathbb{R}^d}{\\text{argmax}} \\: \\log L(w) = \\underset{w \\in \\mathbb{R}^d}{\\text{argmax}} \\: \\sum_{i=1}^n \\left[ y_i \\log\\left(f_{w}(x_i)\\right) + (1 - y_i) \\log\\left( 1 - f_{w}(x_i)\\right) \\right] $$\n",
    "\n",
    "This method is known in statistics as Maximum Likelihood Estimation (MLE) and estimators generated via this method are known as MLE estimators.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to realize that the MLE method is entirely compatible with our Empirical Risk Miniminization theoretical framework. In fact maximizing the logarithm of the Maximum Likelihood function, $\\log(L(w))$, is equivalent to minimizing the log-loss function, defined as follows:\n",
    "\n",
    "$$ \\tilde{L}(y, y') =   - \\left[ y \\log\\left( y' \\right) + (1 - y) \\log\\left( 1 - y'\\right) \\right], \\quad y\\in \\{0,1\\}, y' \\in (0,1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression: the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimization can be performed via gradient descent. The gradient is easily computed as:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial w_k} \\left( - \\log L(w) \\right) = - \\sum_{i=1}^n x_i^{(k)} (y_i - f_{w}(x_i)) =: - \\sum_{i=1}^n x_i^{(k)} z_i(w)  \\quad \\Rightarrow \\quad \\nabla \\log L(w) = - X^T z(w), $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the iterating update rule for our gradient descent will look like:\n",
    "\n",
    "$$ w^{\\text{new}} = w^{\\text{old}} - \\alpha \\left( - X^T z(w,b)\\right),$$\n",
    "\n",
    "where the gradients are taken with negative sign because the optimization problem at hand is a minimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(w, x):\n",
    "    return 1 / (1 + np.exp(-w.dot(x)))\n",
    "\n",
    "def log_likelihood(w, x, y):\n",
    "    n = x.shape[0]\n",
    "    result = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        result[i] = - (1 - y[i,0]) * w.dot(x[i,:]) - np.log(1 + np.exp(-w.dot(x[i,:]))) \n",
    "        # the previous line is equivalent to:\n",
    "        # result[i] = y[i,0] * np.log(sigmoid(w, x[i,:])) + (1 - y[i,0]) * np.log(1 - sigmoid(w, x[i,:]))\n",
    "    return sum(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_train(x, y, alpha=None):\n",
    "    \n",
    "    n, d = x.shape\n",
    "    b = 0\n",
    "    w = np.zeros(d)\n",
    "    cost_old = 0\n",
    "    cost_new = log_likelihood(w, x, y) \n",
    "    i = 0\n",
    "    while np.abs(cost_new - cost_old) > 10 ** (-4):\n",
    "        print(i)\n",
    "        print(cost_new)\n",
    "        z = y - np.array([sigmoid(w, row) for row in x]).reshape((n,1))\n",
    "        gradient = - np.transpose(x).dot(z).reshape((d,))\n",
    "        alpha = (np.transpose(gradient).dot(gradient)) / (\n",
    "            (np.transpose(gradient).dot(np.transpose(x))).dot(x.dot(gradient)))\n",
    "        w = w - alpha * gradient\n",
    "        cost_new, cost_old = log_likelihood(w, x, y) , cost_new\n",
    "        i = i + 1\n",
    "    print(\"Iterations: {}\".format(i))\n",
    "    return w\n",
    "\n",
    "\n",
    "def logistic_regression_predict_label(w, x):\n",
    "    n = x.shape[0]\n",
    "    predictions = np.zeros((n, 1))\n",
    "    for i in range(n):\n",
    "        prob = sigmoid(w, x[i,:])\n",
    "        if prob >= 0.5:\n",
    "            predictions[i,0] = 1\n",
    "        else:\n",
    "            predictions[i,0] = 0\n",
    "    return predictions\n",
    "\n",
    "def logistic_regression_predict_prob(w, x):\n",
    "    n = x.shape[0]\n",
    "    predictions = np.zeros((n, 1))\n",
    "    for i in range(n):\n",
    "        predictions[i,0] = sigmoid(w, x[i,:])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression: practical implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to implement logistic regression on a binary classification problem. The only thing we need is a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset acquisition: how to read text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work on the **Titanic dataset**. This is an online dataset ([see here](https://www.kaggle.com/c/titanic/data) for an online repository) that contains data about the passengers of the Titanic, together with the information whether they survived or not. The goal is to predict, as well as possible, the fate of each passanger by using the information provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes as a csv file (comma-separated values file), which looks as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked  \n",
    "    1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S  \n",
    "    2,1,1,\"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",female,38,1,0,PC 17599,71.2833,C85,C  \n",
    "    3,1,3,\"Heikkinen, Miss. Laina\",female,26,0,0,STON/O2. 3101282,7.925,,S  \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you save the file in the same folder as your Jupyter notebook or python code, your program can access it via several possible python functions. \n",
    "\n",
    "We will use the most basic (and reliable) method, which is via the csv.reader function of the csv package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "with open('train.csv', 'r') as f:\n",
    "    data = csv.reader(f)\n",
    "\n",
    "    row = data.__next__()\n",
    "    features_names = np.array(row)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for row in data:\n",
    "        x.append(row)\n",
    "        y.append(row[1])\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have an idea of the dataset by printing the header (which contains the names of the columns) and the first line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(features_names)\n",
    "print(x[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are as follows:\n",
    "\n",
    "0. '**PassengerId**': a progressive numbering of the passangers (integer)\n",
    "1. '**Survived**': survival status (integer: 0 if dead, 1 if survived)\n",
    "2. '**Pclass**': passenger class (integer: 1, 2, or 3 if 1st, 2nd, or 3rd class respectively)\n",
    "3. '**Name**': name of the passenger (string)\n",
    "4. '**Sex**': gender of the passenger (string: 'male' or 'female')\n",
    "5. '**Age**': age of the passanger (integer)\n",
    "6. '**SibSp**': number of siblings/spouses on board (integer)\n",
    "7. '**Parch**': number of parents/children on board (integer)\n",
    "8. '**Ticket**': string specifying the ticket code (alphanumeric string)\n",
    "9. '**Fare**': cost of the ticket (float)\n",
    "10. '**Cabin**': personal cabin number  (alphanumeric string)\n",
    "11. '**Embarked**': port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that:\n",
    "\n",
    "+ the 'Survived' column is the column of our labels, it's not a feature!\n",
    "+ some features can be meaningful, but cannot be readily translated into real numbers,\n",
    "+ not all features appear to be useful.\n",
    "\n",
    "We decide to limit ourselves to the following features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = x[:, [2, 4, 5, 6, 7, 9]]\n",
    "features_names = features_names[[2, 4, 5, 6, 7, 9]]\n",
    "print(features_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our (restricted) dataset now looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must turn the gender feature into a categorical one (0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x[:,1] = (x[:,1] == 'female').astype(np.float)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we try to convert all data to type float, so we can work with \"real\" numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = x.astype(np.float)\n",
    "y = y.astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion to float fails because some features have no value (represented in text usually as '', 'NaN', or 'NA'). We can search for the culprit in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(features_names)):\n",
    "    if any(x[:,i] == ''):\n",
    "        print(\"Feature\", i, \"has\", sum(x[:,i] == ''), \"NaN value(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can deal with missing data in one of the following ways:\n",
    "+ Remove the datapoint (not good if the dataset is small)\n",
    "+ Use a machine learning algorithm that accepts missing features (but that's not the case of logistic regression)\n",
    "+ **Data imputation** techniques, i.e. substitute the NaN with a plausible value: \n",
    "    + the mean over non-NaN data of same feature, \n",
    "    + the mid-point of the range of non-NaN data range for same feature, \n",
    "    + the prediction of a regression problem run on the remaining features to predict missing feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the simplest solution and substitute the missing value with the mean value of the feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = x[:,2]\n",
    "mean_age = np.mean(x[ages != '',2].astype(np.float))\n",
    "x[ages == '', 2] = mean_age\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.astype(np.float)\n",
    "y = y.astype(np.float).reshape((x.shape[0],1))\n",
    "\n",
    "x = np.hstack((np.ones((x.shape[0],1)), x))\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual we shuffle the dataset and partition it into a training dataset and a testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(x, y):\n",
    "    z = np.hstack((x, y))\n",
    "    np.random.shuffle(z)\n",
    "    return np.hsplit(z, [x.shape[1]])\n",
    "\n",
    "x, y = shuffle(x, y)\n",
    "\n",
    "def splitting(x, y, test_size=0.2):\n",
    "    n = x.shape[0]\n",
    "    train_size = int(n * (1 - test_size))\n",
    "    return x[:train_size, ], x[train_size:, ], y[:train_size, ], y[train_size:, ]\n",
    "\n",
    "x_train, x_test, y_train, y_test = splitting(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logistic_coeff = logistic_regression_train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two loss functions:\n",
    "\n",
    "+ the 0-1 loss (percentage of wrong labels)\n",
    "+ the log-loss (as defined above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_one_loss(y_true, y_pred):\n",
    "    n = y_true.shape[0]\n",
    "    return (1/n) * np.sum(y_true != y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y_true, y_pred):\n",
    "    return np.mean(- (y_true*(np.log(y_pred)) + (1 - y_true)*(np.log(1 - y_pred))))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test our model on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predicted_labels = logistic_regression_predict_label(logistic_coeff, x_train)\n",
    "train_predicted_prob = logistic_regression_predict_prob(logistic_coeff, x_train)\n",
    "test_predicted_labels = logistic_regression_predict_label(logistic_coeff, x_test)\n",
    "test_predicted_prob = logistic_regression_predict_prob(logistic_coeff, x_test)\n",
    "\n",
    "print('Train 0-1 loss:', zero_one_loss(y_train, train_predicted_labels))\n",
    "print('Train log-loss:', log_loss(y_train, train_predicted_prob))\n",
    "print('-----------------------------------------------------------------------')\n",
    "print('Test 0-1 loss:', zero_one_loss(y_test, test_predicted_labels))\n",
    "print('Test log-loss:', log_loss(y_test, test_predicted_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.hstack((y_test, test_predicted_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this error with the implementation of logistic regression of the scikit-learn package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(solver='liblinear').fit(x_train[:,1:], y_train.reshape(y_train.shape[0],))\n",
    "\n",
    "\n",
    "train_predicted_labels = clf.predict(x_train[:,1:]).reshape(y_train.shape)\n",
    "train_predicted_prob = (clf.predict_proba(x_train[:,1:])[:,1]).reshape(y_train.shape)\n",
    "test_predicted_labels = clf.predict(x_test[:,1:]).reshape(y_test.shape)\n",
    "test_predicted_prob = (clf.predict_proba(x_test[:,1:])[:,1]).reshape(y_test.shape)\n",
    "\n",
    "print('Train 0-1 loss:', zero_one_loss(y_train, train_predicted_labels))\n",
    "print('Train log-loss:', log_loss(y_train, train_predicted_prob))\n",
    "print('-----------------------------------------------------------------------')\n",
    "print('Test 0-1 loss:', zero_one_loss(y_test, test_predicted_labels))\n",
    "print('Test log-loss:', log_loss(y_test, test_predicted_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the code yourself! Possible ideas that might lead you to interesting observations are:\n",
    "\n",
    "1. Learn from our results! What features of our datasets are statistically significant for survival prediction?\n",
    "2. Try the algorithms on the dataset for breast cancer diagnosis from scikit-learn (see [here](https://scikit-learn.org/stable/datasets/index.html)).\n",
    "3. Try selecting more or less features. How is the model performance affected?\n",
    "4. Try using different data imputation techniques to substitute the missing \"Age\" values.\n",
    "5. Implement a logistic regression on the Titanic dataset, but this time choose a feature other than \"Survived\" as label.\n",
    "6. Implement an algorithm similar to what we have seen for logistic regression, but on the following hypothesis class:\n",
    "$$\\{x\\mapsto \\text{sign}\\left(\\langle w,x\\rangle \\right) \\;\\vert\\; w\\in\\mathbb{R}^d\\}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
